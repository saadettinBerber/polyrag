# PolyRAG - ModÃ¼ler RAG Framework

**PolyRAG**, esnek, modÃ¼ler ve yÃ¼ksek performanslÄ± bir RAG (Retrieval-Augmented Generation) framework'Ã¼dÃ¼r. Hexagonal Architecture (Ports and Adapters) prensiplerine gÃ¶re tasarlanmÄ±ÅŸ olup, farklÄ± LLM, Vector Database ve Embedding saÄŸlayÄ±cÄ±larÄ± arasÄ±nda kolayca geÃ§iÅŸ yapmanÄ±zÄ± saÄŸlar.

## ğŸš€ Ã–zellikler

*   **Ã‡oklu LLM DesteÄŸi**: Ollama, OpenAI, Claude, Gemini gibi popÃ¼ler modellerle entegrasyon.
*   **Esnek VeritabanÄ±**: Qdrant (VektÃ¶r) ve Neo4j (Graph) veritabanÄ± desteÄŸi.
*   **Multimodal Yetenekler**: Metin, GÃ¶rsel ve Tablo verileriyle Ã§alÄ±ÅŸabilme.
*   **GeliÅŸmiÅŸ Retrieval**: ColBERT, ColPali ve Hybrid arama teknikleri.
*   **Hexagonal Mimari**: BaÄŸÄ±mlÄ±lÄ±klarÄ± izole eden, test edilebilir ve sÃ¼rdÃ¼rÃ¼lebilir kod yapÄ±sÄ±.
*   **Streaming**: Token-by-token yanÄ±t Ã¼retimi.

---

## ğŸ› ï¸ Kurulum

PolyRAG'i kullanmaya baÅŸlamak iÃ§in Ã¶ncelikle Python 3.10 veya Ã¼zeri bir sÃ¼rÃ¼me ihtiyacÄ±nÄ±z vardÄ±r.

1.  **Projeyi KlonlayÄ±n:**
    ```bash
    git clone https://github.com/polyrag/polyrag.git
    cd polyrag
    ```

2.  **Sanal Ortam OluÅŸturun (Ã–nerilen):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # Windows: venv\Scripts\activate
    ```

3.  **BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Not: GeliÅŸtirme modunda kurulum iÃ§in `pip install -e .` komutunu kullanabilirsiniz.)*

4.  **Gerekli Servisleri AyaÄŸa KaldÄ±rÄ±n:**
    Ã–rnekleri Ã§alÄ±ÅŸtÄ±rmak iÃ§in Ollama ve Qdrant'Ä±n yerel makinenizde Ã§alÄ±ÅŸÄ±yor olmasÄ± gerekir.
    *   **Ollama:** [ollama.com](https://ollama.com) adresinden indirin.
    *   **Qdrant:** Docker ile hÄ±zlÄ±ca baÅŸlatÄ±n:
        ```bash
        docker run -p 6333:6333 qdrant/qdrant
        ```

---

## âš¡ HÄ±zlÄ± BaÅŸlangÄ±Ã§

AÅŸaÄŸÄ±daki Ã¶rnek, basit bir metin belgesi Ã¼zerinden RAG akÄ±ÅŸÄ±nÄ±n nasÄ±l oluÅŸturulacaÄŸÄ±nÄ± gÃ¶sterir.

```python
from polyrag.interface.builder import PipelineBuilder
from polyrag.interface.factory import AdapterFactory

def main():
    # 1. Adapter'larÄ± OluÅŸturun
    llm = AdapterFactory.create_llm("ollama", model="llama3.2")
    embedding = AdapterFactory.create_embedding("fastembed")
    vector_store = AdapterFactory.create_vector_store("qdrant")
    loader = AdapterFactory.create_document_loader("text")
    chunker = AdapterFactory.create_chunker("fixed_size", chunk_size=500, chunk_overlap=50)

    # 2. Pipeline'Ä± Ä°nÅŸa Edin
    pipeline = (
        PipelineBuilder()
        .with_llm(llm)
        .with_embedding(embedding)
        .with_vector_store(vector_store)
        .with_document_loader(loader)
        .with_chunker(chunker)
        .with_collection_name("my_rag_collection")
        .build()
    )

    # 3. Veri YÃ¼kleyin (Ingestion)
    # 'data.txt' adÄ±nda bir dosyanÄ±z olduÄŸunu varsayalÄ±m.
    pipeline.ingest("data.txt")

    # 4. Soru Sorun (Querying)
    question = "Bu belgenin ana fikri nedir?"
    print(f"Soru: {question}\nCevap:")
    
    for chunk in pipeline.query_stream(question):
        print(chunk, end="", flush=True)

if __name__ == "__main__":
    main()
```

---

## ğŸ—ï¸ DetaylÄ± Mimari

PolyRAG, **Hexagonal Architecture** (Ports ve Adapters) yapÄ±sÄ±nÄ± benimser. Bu mimari, iÅŸ mantÄ±ÄŸÄ±nÄ± (Core Domain) dÄ±ÅŸ dÃ¼nyadan (VeritabanlarÄ±, API'ler, Framework'ler) ayÄ±rÄ±r.

### Temel Katmanlar

1.  **Interface Layer (ArayÃ¼z KatmanÄ±)**:
    *   KullanÄ±cÄ±nÄ±n sistemle etkileÅŸime girdiÄŸi yerdir.
    *   `PipelineBuilder`: AkÄ±ÅŸkan (fluent) bir arayÃ¼z ile pipeline oluÅŸturmayÄ± saÄŸlar.
    *   `PolyRAGPipeline`: Ingestion ve Query sÃ¼reÃ§lerini yÃ¶neten ana orkestratÃ¶rdÃ¼r.

2.  **Core Domain (Ã‡ekirdek Katman)**:
    *   Sistemin kalbidir. HiÃ§bir dÄ±ÅŸ kÃ¼tÃ¼phaneye baÄŸÄ±mlÄ± deÄŸildir.
    *   **Ports (ArayÃ¼zler)**: `LLMPort`, `VectorStorePort` gibi soyut sÄ±nÄ±flarÄ± tanÄ±mlar. Adapter'lar bu portlarÄ± implemente etmek zorundadÄ±r.
    *   **Models**: `Document`, `Chunk`, `RetrievalResult` gibi veri yapÄ±larÄ±nÄ± iÃ§erir.

3.  **Adapters Layer (AdaptÃ¶r KatmanÄ±)**:
    *   DÄ±ÅŸ teknolojilerle Core katmanÄ± arasÄ±ndaki kÃ¶prÃ¼dÃ¼r.
    *   Ã–rneÄŸin: `OllamaAdapter`, `QdrantAdapter`, `FastEmbedAdapter`.
    *   Yeni bir teknoloji eklemek iÃ§in sadece yeni bir adapter yazmak yeterlidir; Core kodunu deÄŸiÅŸtirmeye gerek yoktur.

### Mimari ÅemasÄ±

```mermaid
graph TD
    User[KullanÄ±cÄ± / Uygulama] --> Interface[Interface Layer\n(Pipeline, Builder)]
    
    subgraph Core Domain
        Ports[Ports\n(Abstract Interfaces)]
        Models[Domain Models\n(Document, Chunk)]
    end
    
    Interface --> Ports
    Interface --> Models
    
    subgraph Adapters Layer
        LLM[LLM Adapter\n(Ollama, OpenAPI)]
        VectorDB[Vector Store\n(Qdrant, Chroma)]
        Embed[Embedding\n(FastEmbed, OpenAI)]
        Loader[Doc Loader\n(PDF, Text)]
    end
    
    LLM -. implements .-> Ports
    VectorDB -. implements .-> Ports
    Embed -. implements .-> Ports
    Loader -. implements .-> Ports
```

### AkÄ±ÅŸ DiyagramlarÄ±

**Ingestion (Veri YÃ¼kleme) AkÄ±ÅŸÄ±:**
`Dosya` -> `Document Loader` -> `Chunker` -> `Embedder` -> `Vector Store`

**Query (Sorgulama) AkÄ±ÅŸÄ±:**
`Soru` -> `Embedder` -> `Retriever (Vector Store)` -> `Reranker (Opsiyonel)` -> `Context Builder` -> `LLM`

---

## ğŸ“‚ Proje YapÄ±sÄ±

```
polyrag/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ ports/           # TÃ¼m soyut arayÃ¼zler (LLMPort, vb.)
â”‚   â”œâ”€â”€ models/          # Veri sÄ±nÄ±flarÄ± (Document, Chunk, vb.)
â”‚   â””â”€â”€ services/        # Temel servis mantÄ±klarÄ±
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ llm/             # LLM implementasyonlarÄ±
â”‚   â”œâ”€â”€ embedding/       # Embedding model entegrasyonlarÄ±
â”‚   â”œâ”€â”€ vector_store/    # VektÃ¶r veritabanÄ± sÃ¼rÃ¼cÃ¼leri
â”‚   â”œâ”€â”€ document_loader/ # Dosya okuyucular
â”‚   â””â”€â”€ chunking/        # Metin parÃ§alama algoritmalarÄ±
â”œâ”€â”€ interface/
â”‚   â”œâ”€â”€ pipeline.py      # Ana Ã§alÄ±ÅŸma mantÄ±ÄŸÄ±
â”‚   â””â”€â”€ builder.py       # Pipeline oluÅŸturucu
â””â”€â”€ examples/            # Ã–rnek senaryolar
```
